{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87880bd-2ab8-49e3-a835-f5b4da224c9a",
   "metadata": {},
   "source": [
    "Spam, or unsolicited bulk messaging, has been a persistent issue since the early days of digital communication. It clutters inboxes, poses security risks, and can be used for malicious purposes such as phishing attacks. Effective spam detection is crucial for maintaining the integrity and usability of email systems and other messaging platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e55f2-58d2-498d-a71a-da419475f0ff",
   "metadata": {},
   "source": [
    "The Spam Dataset : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d190d04-386c-4124-97a8-12aeaac409e2",
   "metadata": {},
   "source": [
    "We'll explore Bayesian spam classification using the SMS Spam Collection dataset, a curated resource tailored for developing and evaluating text-based spam filters. This dataset emerges from the combined efforts of Tiago A. Almeida and Akebo Yamakami at the School of Electrical and Computer Engineering at the University of Campinas in Brazil, and José María Gómez Hidalgo at the R&D Department of Optenet in Spain.\n",
    "\n",
    "Their work, \"Contributions to the Study of SMS Spam Filtering: New Collection and Results,\" presented at the 2011 ACM Symposium on Document Engineering, aimed to address the growing problem of unsolicited mobile phone messages, commonly known as SMS spam. Recognizing that many existing spam filtering resources focused on email rather than text messages, the authors assembled this dataset from multiple sources, including the Grumbletext website, the NUS SMS Corpus, and Caroline Tag’s PhD thesis.\n",
    "\n",
    "The resulting corpus contains 5,574 text messages annotated as either ham (legitimate) or spam (unwanted), making it a great resource for building and testing models that can differentiate meaningful communications from intrusive or deceptive ones. In this context, ham refers to messages from known contacts, subscriptions, or newsletters that hold value for the recipient, while spam represents unsolicited content that typically offers no benefit and may even pose risks to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe40c1c-2a7d-4a14-92b8-e3e621dfef51",
   "metadata": {},
   "source": [
    "Downloading Loading and Exploring the Dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0e924be-0db4-4bbf-9f03-f870548d0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# Downloading the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download successful\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ebc01a0-9c55-4792-9a99-1ea257e991a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction successful\n"
     ]
    }
   ],
   "source": [
    "# Extracting the dataset\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"sms_spam_collection\")\n",
    "    print(\"Extraction successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "184859c4-0e94-4acb-b652-627c2bd55c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(\"sms_spam_collection\")\n",
    "print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a44e2fff-2956-4933-aa08-1d2c945a8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"sms_spam_collection/SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"message\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa461478-aa7d-4f8d-b7ae-43aafbc0e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- HEAD --------------------\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "-------------------- DESCRIBE --------------------\n",
      "       label                 message\n",
      "count   5572                    5572\n",
      "unique     2                    5169\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n",
      "-------------------- INFO --------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"-------------------- HEAD --------------------\")\n",
    "print(df.head())\n",
    "print(\"-------------------- DESCRIBE --------------------\")\n",
    "print(df.describe())\n",
    "print(\"-------------------- INFO --------------------\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "190c5104-5723-4e51-81c0-d0df9e5490ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "Duplicate entries: 403\n"
     ]
    }
   ],
   "source": [
    "#few checks : \n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum()) \n",
    "# Check for duplicates\n",
    "print(\"Duplicate entries:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ba2983e-d018-4bf2-945f-0ce1f85ecefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fbb47-3d77-45d2-a6e3-2f534cfdd5df",
   "metadata": {},
   "source": [
    "Preprocessing the Spam Dataset : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359139e9-048b-40e5-b4dd-a538e36fb194",
   "metadata": {},
   "source": [
    "After loading the SMS Spam Collection dataset, the next step is preprocessing the text data. Preprocessing standardizes the text, reduces noise, and extracts meaningful features, all of which improve the performance of the Bayes spam classifier. The steps outlined here rely on the nltk library for tasks such as tokenization, stop word removal, and stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46432934-8a8a-4f12-990f-bf048fedde70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSING ===\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\besse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\besse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\besse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSING ===\") \n",
    "print(df.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295245e3-dcdf-4675-bf9f-a3e0d1ba571c",
   "metadata": {},
   "source": [
    "Lowercasing the Text : \n",
    "Lowercasing the text ensures that the classifier treats words equally, regardless of their original casing. By converting all characters to lowercase, the model considers \"Shahed\" and \"shahed\" as the same token, effectively reducing dimensionality and improving consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed8fc6e8-df39-4d7f-b894-31af810a6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER LOWERCASING ===\n",
      "<bound method NDFrame.head of      label                                            message\n",
      "0      ham  go until jurong point, crazy.. available only ...\n",
      "1      ham                      ok lar... joking wif u oni...\n",
      "2     spam  free entry in 2 a wkly comp to win fa cup fina...\n",
      "3      ham  u dun say so early hor... u c already then say...\n",
      "4      ham  nah i don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5567  spam  this is the 2nd time we have tried 2 contact u...\n",
      "5568   ham               will ü b going to esplanade fr home?\n",
      "5569   ham  pity, * was in mood for that. so...any other s...\n",
      "5570   ham  the guy did some bitching but i acted like i'd...\n",
      "5571   ham                         rofl. its true to its name\n",
      "\n",
      "[5169 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#lowercasing the text \n",
    "df[\"message\"]=df[\"message\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efe52bda-2a0a-4d21-8b8a-6a6062d0c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
      "  label                                            message\n",
      "0   ham  go until jurong point crazy available only in ...\n",
      "1   ham                            ok lar joking wif u oni\n",
      "2  spam  free entry in  a wkly comp to win fa cup final...\n",
      "3   ham        u dun say so early hor u c already then say\n",
      "4   ham  nah i dont think he goes to usf he lives aroun...\n"
     ]
    }
   ],
   "source": [
    "# Removing non-essential punctuation and numbers, keep useful symbols like $ and !\n",
    "import re\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016188d9-7ec0-4046-b989-82a3addbaa87",
   "metadata": {},
   "source": [
    "### Tokenizing the text : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "197ece1c-9966-4a47-aeb3-e1f7be746a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER TOKENIZATION ===\n",
      "0    [go, until, jurong, point, crazy, available, o...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
      "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
      "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each message into individual tokens\n",
    "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
    "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae3f1d90-da71-4ff4-88f1-fe6a2c0acbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING STOP WORDS ===\n",
      "0    [go, jurong, point, crazy, available, bugis, n...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, early, hor, u, c, already, say]\n",
      "4    [nah, dont, think, goes, usf, lives, around, t...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them from the tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2a9d2-43aa-497a-8242-5935b618e206",
   "metadata": {},
   "source": [
    "## Stemming : \n",
    "Stemming normalizes words by reducing them to their base form (e.g., running becomes run). This consolidates different forms of the same root word, effectively cutting the vocabulary size and smoothing out the text representation. As a result, the model can better understand the underlying concepts without being distracted by trivial variations in word forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda03cf-5d92-478a-8550-b6d4ddb5fd40",
   "metadata": {},
   "source": [
    "# Rejoining back the tokens into a space seperated sentence : \n",
    "While some ML models don't need this and only require tokens for building , it's still a good idea to rejoin the tokens into a space-separated string restores a format compatible with these methods, allowing the dataset to move seamlessly into the feature extraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b3e35b8-c4c3-43e2-9916-ca448f046b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
      "0    go jurong point crazy available bugis n great ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry wkly comp win fa cup final tkts st ...\n",
      "3                  u dun say early hor u c already say\n",
      "4          nah dont think goes usf lives around though\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
    "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe8d04-a82f-43b2-9a2c-07c8276e2e23",
   "metadata": {},
   "source": [
    "At this point, the messages are fully preprocessed. Each message is a cleaned, normalized string ready for vectorization and subsequent model training, ultimately improving the classifier’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ccfb61-fce5-4cce-9aa1-8cc83caf7a25",
   "metadata": {},
   "source": [
    "# Feature Extraction : \n",
    "CountVectorizer from the scikit-learn library efficiently implements the bag-of-words approach. It converts a collection of documents into a matrix of term counts, where each row represents a message and each column corresponds to a term (unigram or bigram). Before transformation, CountVectorizer applies tokenization, builds a vocabulary, and then maps each document to a numeric vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8521abfa-f0b2-4459-9032-c9d62b574a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n",
    "\n",
    "# Labels (target variable)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00dd06-8bff-423d-89f1-7e937500e25b",
   "metadata": {},
   "source": [
    "# Training and Evaluation of the Model : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657025f-b39c-4c01-a42b-344a70368510",
   "metadata": {},
   "source": [
    "##### Training : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc56459-6451-4f7a-b2f2-89b420f50f7a",
   "metadata": {},
   "source": [
    "After preprocessing the text data and extracting meaningful features, we train a machine-learning model for spam detection. We use the Multinomial Naive Bayes classifier, which is well-suited for text classification tasks due to its probabilistic nature and ability to efficiently handle large, sparse feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0cac3-6c1d-4a2a-8060-a55d8334b89c",
   "metadata": {},
   "source": [
    "To streamline the entire process, we employ a Pipeline. A pipeline chains together the vectorization and modeling steps, ensuring that the same data transformation (in this case, CountVectorizer) is consistently applied before feeding the transformed data into the classifier. This approach simplifies both development and maintenance by encapsulating the feature extraction and model training into a single, unified workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcfefcf3-4e91-47e4-8458-cf23225c7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline by combining vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ace3b4d-3ba3-4579-be74-73268a7a88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'classifier__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80e853-a245-4472-8976-38ac7cb75715",
   "metadata": {},
   "source": [
    "# Evaluation and Testing on new messages : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "986af9db-17db-4c64-94b7-eb3a61f14652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.97\n",
      "Not-Spam Probability: 0.03\n",
      "--------------------------------------------------\n",
      "Message: Hey, are we still meeting up for lunch today?\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.94\n",
      "Not-Spam Probability: 0.06\n",
      "--------------------------------------------------\n",
      "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.01\n",
      "Not-Spam Probability: 0.99\n",
      "--------------------------------------------------\n",
      "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example SMS messages for evaluation\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]\n",
    "#Before predicting with the trained model, we must preprocess the new messages using the same steps applied during training. Consistent preprocessing ensures that the model receives data in the same format it was trained on\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "# Preprocess function that mirrors the training-time preprocessing\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "# Preprocess and vectorize messages\n",
    "processed_messages = [preprocess_message(msg) for msg in new_messages]    \n",
    "# same with Vectorizing the Processed Messages : \n",
    "# Transform preprocessed messages into feature vectors\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)\n",
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)\n",
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
    "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a7eda-ddd1-48a2-b55e-283c65c16f29",
   "metadata": {},
   "source": [
    "# Using joblib for Saving our  Model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68d08bf8-d2af-4e92-a970-f0ac93abac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'spam_detection_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "loaded_model = joblib.load(model_filename)\n",
    "predictions = loaded_model.predict(new_messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
